#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ArtifactsBenchæ•°æ®å¤„ç†è„šæœ¬
è¯»å–artifacts_bench.jsonlæ–‡ä»¶ï¼Œè°ƒç”¨å¤šä¸ªLLMæ¨¡å‹è·å–å›å¤

è¾“å‡ºæ•°æ®ç»“æ„è¯´æ˜ï¼š
æ¯æ¡è®°å½•åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- index: åŸå§‹æ•°æ®ç´¢å¼•
- question: åŸå§‹é—®é¢˜
- model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
- response: æ¨¡å‹å›å¤
- input_tokens: è¾“å…¥tokenæ•°é‡
- output_tokens: è¾“å‡ºtokenæ•°é‡
- success: æ˜¯å¦æˆåŠŸ
- processed_at: å¤„ç†æ—¶é—´
"""

import json
import os
import sys
import time
import random
import logging
from datetime import datetime
from http import HTTPStatus
from openai import OpenAI, RateLimitError, InternalServerError, APITimeoutError
import ast
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

def setup_logging(log_file):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        filename=log_file,
        level=logging.ERROR,
        format='%(asctime)s-%(levelname)s-%(message)s'
    ) 

def save_json(save_data, save_path):
    """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    # å†™å…¥JSONæ–‡ä»¶
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)

def read_json(path):
    """è¯»å–JSONæ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def read_jsonl(path):
    """è¯»å–JSONLæ–‡ä»¶"""
    data = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    # å°è¯•è§£æä¸ºJSON
                    item = json.loads(line)
                    data.append(item)
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç”¨ast.literal_evalè§£æPythonå­—å…¸æ ¼å¼
                    try:
                        item = ast.literal_eval(line)
                        data.append(item)
                    except:
                        print(f"æ— æ³•è§£æè¡Œ: {line}")
                        continue
    return data

def save_jsonl(data, path):
    """ä¿å­˜JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def load_existing_results(output_file_prefix, model_names):
    """åŠ è½½å·²å­˜åœ¨çš„ç»“æœï¼Œè¿”å›æ¯ä¸ªæ¨¡å‹å·²å¤„ç†çš„æ•°æ®IDé›†åˆ"""
    existing_results = {}
    
    for model_name in model_names:
        output_file = f"{output_file_prefix}_{model_name}.jsonl"
        processed_ids = set()
        
        if os.path.exists(output_file):
            try:
                results = read_jsonl(output_file)
                # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šartifacts_benchæ ¼å¼ä½¿ç”¨indexï¼ŒåŸæ ¼å¼ä½¿ç”¨custom_id
                processed_ids = {str(item.get('index', item.get('custom_id', ''))) for item in results if item.get('index') or item.get('custom_id')}
                print(f"å‘ç° {model_name} å·²å¤„ç† {len(processed_ids)} æ¡æ•°æ®")
            except Exception as e:
                print(f"è¯»å– {model_name} ç°æœ‰ç»“æœæ—¶å‡ºé”™: {e}")
        
        existing_results[model_name] = processed_ids
    
    return existing_results

class ChatBot(object):
    """
    èŠå¤©æœºå™¨äººç±»ï¼Œæ”¯æŒå¤šç§APIç±»å‹
    apiå‚æ•°æ ¼å¼ï¼š
    {
        "type": "OPENAI" | "DASHSCOPE" | "zhipu",
        "base": "API base URL",
        "key": "API key",
        "engine": "æ¨¡å‹åç§°",
        "max_tokens": è¾“å‡ºé•¿åº¦é™åˆ¶ï¼ˆå¯é€‰ï¼‰
    }
    """
    def __init__(self, api, max_try=10, max_tokens=4096, tem=0.000001) -> None:
        self.mode = api["type"]
        self.model = api["engine"]
        # self.max_tokens = max_tokens
        self.max_tokens = 8096
        self.max_try = max_try
        self.tem = tem
        self.key = api["key"]
        
        # å¦‚æœAPIä¸­æŒ‡å®šäº†max_tokensåˆ™ä½¿ç”¨APIä¸­çš„å€¼
        if "max_tokens" in api.keys():
            self.max_tokens = api["max_tokens"]
        
        # åˆå§‹åŒ–ä¸åŒç±»å‹çš„å®¢æˆ·ç«¯
        if self.mode == "OPENAI":
            self.client = OpenAI(api_key=api["key"], base_url=api["base"])
    
    def call(self, messages, test=False):
        """è°ƒç”¨èŠå¤©APIï¼Œè¿”å›(response_content, input_tokens, output_tokens)"""
        if self.mode in ["OPENAI"]:
            return self.call_openai(messages, test)
    
    def call_openai(self, messages, test=False):
        """è°ƒç”¨OpenAI APIï¼Œè¿”å›(response_content, input_tokens, output_tokens)"""
        for attempt in range(self.max_try):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.tem,
                    max_tokens=self.max_tokens
                )
                
                # è·å–å“åº”å†…å®¹å’Œtokenä½¿ç”¨ä¿¡æ¯
                response_content = response.choices[0].message.content
                input_tokens = response.usage.prompt_tokens if response.usage else 0
                output_tokens = response.usage.completion_tokens if response.usage else 0
                
                return response_content, input_tokens, output_tokens
                
            except (RateLimitError, InternalServerError, APITimeoutError) as e:
                if attempt < self.max_try - 1:
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    print(f"APIé”™è¯¯ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•ï¼Œç­‰å¾…{wait_time:.2f}ç§’: {e}")
                    time.sleep(wait_time)
                else:
                    print(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{self.max_try}æ¬¡: {e}")
                    return None, 0, 0
            except Exception as e:
                print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
                return None, 0, 0

def call_single_model_artifacts(model_name, chat_bot, messages, custom_id, original_data, max_retry=3):
    """é€‚ç”¨äºartifacts_benchæ•°æ®æ ¼å¼çš„å•ä¸ªæ¨¡å‹è°ƒç”¨å‡½æ•°"""
    thread_id = threading.current_thread().ident
    print(f"[çº¿ç¨‹{thread_id}] å¼€å§‹è°ƒç”¨ {model_name} æ¨¡å‹å¤„ç†æ•°æ® {custom_id}")
    
    for attempt in range(max_retry):
        try:
            result = chat_bot.call(messages)
            
            if result[0] and result[0].strip():  # result[0] æ˜¯å“åº”å†…å®¹
                response_content, input_tokens, output_tokens = result
                
                print(f"[çº¿ç¨‹{thread_id}] {model_name} æ¨¡å‹å¤„ç†æ•°æ® {custom_id} æˆåŠŸ")
                print(f"[çº¿ç¨‹{thread_id}] è¾“å…¥tokens: {input_tokens}, è¾“å‡ºtokens: {output_tokens}")
                
                return {
                    'index': custom_id,
                    'question': original_data['question'],
                    'model_name': model_name,
                    'response': response_content,
                    'input_tokens': input_tokens,
                    'output_tokens': output_tokens,
                    'success': True,
                    'processed_at': datetime.now().isoformat(),
                    'class': original_data.get('class', ''),
                    'difficulty': original_data.get('difficulty', '')
                }
            else:
                print(f"[çº¿ç¨‹{thread_id}] {model_name} ç¬¬ {attempt + 1} æ¬¡å°è¯•è¿”å›ç©ºå“åº”")
                
        except Exception as e:
            print(f"[çº¿ç¨‹{thread_id}] {model_name} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}")
            if attempt < max_retry - 1:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"[çº¿ç¨‹{thread_id}] ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
    
    # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
    print(f"[çº¿ç¨‹{thread_id}] {model_name} å¤„ç†æ•°æ® {custom_id} ç»è¿‡ {max_retry} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
    return {
        'index': custom_id,
        'question': original_data['question'],
        'model_name': model_name,
        'response': None,
        'input_tokens': 0,
        'output_tokens': 0,
        'success': False,
        'processed_at': datetime.now().isoformat(),
        'class': original_data.get('class', ''),
        'difficulty': original_data.get('difficulty', '')
    }

def process_artifacts_bench_file(input_file, output_file_prefix, chat_bots, max_concurrent_requests=6):
    """
    å¤„ç†artifacts_bench.jsonlæ–‡ä»¶ï¼Œä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å¤šæ¡æ•°æ®
    ä¼˜åŒ–å•æ¨¡å‹çš„å¤„ç†é€Ÿåº¦
    """
    # è¯»å–è¾“å…¥æ•°æ®
    print(f"å¼€å§‹è¯»å–æ–‡ä»¶: {input_file}")
    input_data = read_jsonl(input_file)  # è¯»å–JSONLæ–‡ä»¶
    print(f"å…±è¯»å–åˆ° {len(input_data)} æ¡æ•°æ®")
    
    # åŠ è½½å·²å­˜åœ¨çš„ç»“æœ
    existing_results = load_existing_results(output_file_prefix, chat_bots.keys())
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç»“æœåˆ—è¡¨ï¼Œå¹¶åŠ è½½å·²æœ‰ç»“æœ
    model_results = {}
    for model_name in chat_bots.keys():
        output_file = f"{output_file_prefix}_{model_name}.jsonl"
        if os.path.exists(output_file):
            try:
                model_results[model_name] = read_jsonl(output_file)
                print(f"åŠ è½½ {model_name} å·²æœ‰ {len(model_results[model_name])} æ¡ç»“æœ")
            except:
                model_results[model_name] = []
        else:
            model_results[model_name] = []
    
    print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
    print(f"å°†ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå•ç‹¬çš„è¾“å‡ºæ–‡ä»¶:")
    for model_name in chat_bots.keys():
        output_file = f"{output_file_prefix}_{model_name}.jsonl"
        print(f"  {model_name}: {output_file}")
    
    print(f"ä½¿ç”¨ {max_concurrent_requests} ä¸ªå¹¶å‘çº¿ç¨‹å¤„ç†æ•°æ®")
    
    # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤å†™å…¥æ“ä½œ
    write_lock = threading.Lock()
    
    # å‡†å¤‡éœ€è¦å¤„ç†çš„ä»»åŠ¡
    tasks_to_process = []
    skipped_count = 0
    # input_data = [input_data[1458],]
    print(len(input_data),'-----------------------------------------')
    for idx, data in enumerate(input_data):
        try:
            if 'index' not in data or 'question' not in data:
                print(data,'-----------------------------------------')
                print(f"ç¬¬ {idx + 1} æ¡æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡")
                continue
            
            custom_id = str(data['index'])  # ä½¿ç”¨indexä½œä¸ºcustom_id
            question = data['question']
            
            # å°†questionè½¬æ¢ä¸ºmessagesæ ¼å¼
            messages = [{"role": "user", "content": question}]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
            need_process = False
            for model_name in chat_bots.keys():
                if custom_id not in existing_results[model_name]:
                    need_process = True
                    break
            
            if need_process:
                tasks_to_process.append((custom_id, messages, idx + 1, data))
            else:
                print(f"è·³è¿‡æ•°æ® {custom_id}ï¼ˆå·²å¤„ç†è¿‡ï¼‰")
                skipped_count += 1
                
        except Exception as e:
            print(f"å‡†å¤‡ç¬¬ {idx + 1} æ¡æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    print(f"å‡†å¤‡å¤„ç† {len(tasks_to_process)} æ¡æ•°æ®ï¼Œè·³è¿‡ {skipped_count} æ¡")
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰ä»»åŠ¡
    success_count = 0
    failed_count = 0
    
    with ThreadPoolExecutor(max_workers=max_concurrent_requests) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {}
        for custom_id, messages, idx, original_data in tasks_to_process:
            for model_name, chat_bot in chat_bots.items():
                if custom_id not in existing_results[model_name]:
                    future = executor.submit(call_single_model_artifacts, model_name, chat_bot, messages, custom_id, original_data)
                    future_to_task[future] = (model_name, custom_id, idx)
        
        print(f"å·²æäº¤ {len(future_to_task)} ä¸ªä»»åŠ¡åˆ°çº¿ç¨‹æ± ")
        
        # æ”¶é›†ç»“æœ
        completed_tasks = 0
        for future in as_completed(future_to_task):
            model_name, custom_id, idx = future_to_task[future]
            completed_tasks += 1
            
            try:
                result = future.result()
                
                # ä½¿ç”¨é”ä¿æŠ¤å†™å…¥æ“ä½œ
                with write_lock:
                    model_results[model_name].append(result)
                    existing_results[model_name].add(custom_id)
                    
                    # ç«‹å³ä¿å­˜å•ä¸ªæ¨¡å‹çš„ç»“æœ
                    output_file = f"{output_file_prefix}_{model_name}.jsonl"
                    save_jsonl(model_results[model_name], output_file)
                    
                    if result['success']:
                        success_count += 1
                        print(f"âœ… [{completed_tasks}/{len(future_to_task)}] {model_name} å¤„ç†æ•°æ® {custom_id} æˆåŠŸ")
                    else:
                        failed_count += 1
                        print(f"âŒ [{completed_tasks}/{len(future_to_task)}] {model_name} å¤„ç†æ•°æ® {custom_id} å¤±è´¥")
                        
            except Exception as e:
                failed_count += 1
                print(f"âŒ [{completed_tasks}/{len(future_to_task)}] å¤„ç† {model_name} æ•°æ® {custom_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                
                # åˆ›å»ºå¤±è´¥è®°å½•
                failure_result = {
                    'index': custom_id,
                    'question': '',
                    'model_name': model_name,
                    'response': None,
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'success': False,
                    'processed_at': datetime.now().isoformat()
                }
                with write_lock:
                    model_results[model_name].append(failure_result)
                    existing_results[model_name].add(custom_id)
                    
                    output_file = f"{output_file_prefix}_{model_name}.jsonl"
                    save_jsonl(model_results[model_name], output_file)
    
    # ç¡®ä¿æœ€ç»ˆç»“æœå·²ä¿å­˜ï¼ˆå®é™…ä¸Šæ¯ä¸ªå›å¤éƒ½å·²å®æ—¶ä¿å­˜ï¼‰
    print(f"\nâœ… ç¡®ä¿æœ€ç»ˆç»“æœå·²ä¿å­˜...")
    for model_name, results in model_results.items():
        output_file = f"{output_file_prefix}_{model_name}.jsonl"
        save_jsonl(results, output_file)
        print(f"{model_name} æœ€ç»ˆç»“æœç¡®è®¤ä¿å­˜åˆ°: {output_file} (å…± {len(results)} æ¡)")
    
    # æ‰“å°æ€»ç»“
    print(f"\nğŸ‰ å¹¶è¡Œå¤„ç†å®Œæˆï¼")
    print(f"æ€»å…±å¤„ç†: {len(tasks_to_process)} æ¡æ•°æ®")
    print(f"æˆåŠŸ: {success_count} æ¡")
    print(f"å¤±è´¥: {failed_count} æ¡")
    print(f"è·³è¿‡ï¼ˆå·²å¤„ç†ï¼‰: {skipped_count} æ¡")
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“Š å„æ¨¡å‹å¤„ç†ç»Ÿè®¡:")
    for model_name, results in model_results.items():
        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]
        total_input_tokens = sum(r.get('input_tokens', 0) for r in results)
        total_output_tokens = sum(r.get('output_tokens', 0) for r in results)
        print(f"  {model_name}: æˆåŠŸ {len(successful_results)} æ¡, å¤±è´¥ {len(failed_results)} æ¡")
        print(f"    è¾“å…¥tokens: {total_input_tokens}, è¾“å‡ºtokens: {total_output_tokens}")

# ä¸»ç¨‹åº
if __name__ == '__main__':
    # é…ç½®æ¨¡å‹APIs - å•æ¨¡å‹å¹¶è¡Œä¼˜åŒ–
    model_configs = {
        # "deepseek-r1-250528": {
        #     "type": "OPENAI", 
        #     "base": 'https://api-gateway.glm.ai/v1',
        #     "key": "sk-IymdOoiHI3umkLDLVToHmBwyfIKoiWoA",
        #     "engine": "deepseek-r1-250528",
        # },
        # "deepseek-v3-0324": {
        #     "type": "OPENAI", 
        #     "base": 'https://api-gateway.glm.ai/v1',
        #     "key": "sk-IymdOoiHI3umkLDLVToHmBwyfIKoiWoA",
        #     "engine": "deepseek-v3-0324",
        # },
        "qwen3-8b-html-sft_full": {
            "type": "OPENAI", 
            "base": "http://localhost:8000/v1",
            "key": "EMPTY",
            "engine": "/workspace/zhengda/self_learn/output/qwen3-8b-html-sft_full",
        },

    }

    # åˆ›å»ºChatBotå®ä¾‹
    chat_bots = {}
    for model_name, config in model_configs.items():
        chat_bots[model_name] = ChatBot(config)
    
    # æµ‹è¯•è¿æ¥
    print("æµ‹è¯•æ¨¡å‹è¿æ¥...")
    test_messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ä¸€ä¸‹ã€‚"}]
    
    for model_name, chat_bot in chat_bots.items():
        try:
            test_result = chat_bot.call(test_messages)
            if test_result[0]:  # test_result[0] æ˜¯å“åº”å†…å®¹
                print(f"{model_name} è¿æ¥æµ‹è¯•æˆåŠŸ: {test_result[0]}")
                print(f"  æµ‹è¯•æ¶ˆè€—tokens - è¾“å…¥: {test_result[1]}, è¾“å‡º: {test_result[2]}")
            else:
                print(f"{model_name} è¿æ¥æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"{model_name} è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # è®¾ç½®æ–‡ä»¶è·¯å¾„ - è¯»å–artifacts_bench.jsonlæ–‡ä»¶
    input_file = os.path.join(script_dir, "artifacts_bench.jsonl")  # è¾“å…¥æ–‡ä»¶è·¯å¾„
    output_file_prefix = os.path.join(script_dir, "artifacts_results")  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
    
    print(f"è„šæœ¬ç›®å½•: {script_dir}")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶å‰ç¼€: {output_file_prefix}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{input_file}' ä¸å­˜åœ¨")
        sys.exit(1)
    
    # å¼€å§‹å¤„ç† - ä½¿ç”¨10ä¸ªå¹¶å‘çº¿ç¨‹åŠ é€Ÿ
    process_artifacts_bench_file(input_file, output_file_prefix, chat_bots, max_concurrent_requests=10) 